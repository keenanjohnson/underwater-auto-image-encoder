# Configuration for setup_and_train.py
# All-in-one script for downloading, preparing, cropping, and training

# Dataset Download Configuration
download:
  repo_id: "Seattle-Aquarium/Seattle_Aquarium_benthic_imagery"
  dataset_dir: "dataset_raw"  # Directory to download/store raw dataset

# Dataset Preparation Configuration
preparation:
  working_dir: "training_dataset"  # Working directory for prepared dataset
  sets: null  # Specific sets to process (null = all sets). Example: ["set01", "set02"]
  symlink: false  # Use symlinks instead of copying (saves disk space)
  split_ratio: 0.8  # Train/validation split ratio

# Cropping Configuration
cropping:
  input:
    width: 4606  # Crop width for input images
    height: 4030  # Crop height for input images
  output:
    enabled: false  # Skip output cropping (HuggingFace outputs are already consistent)
    width: 4606  # Only used if enabled
    height: 4030  # Only used if enabled

# Training Configuration
training:
  image_size: 512  # Training image size (patches)
  patches_per_image: 4  # Number of random patches to extract per image per epoch (1-8)
                        # Higher values = better coverage but more memory/time per epoch
                        # Patches are re-randomized each epoch for maximum variety
  batch_size: 12  # Training batch size
  epochs: 150  # Number of training epochs
  learning_rate: 0.0002  # Learning rate
  output_dir: "output"  # Output directory for trained models
  checkpoint_dir: "checkpoints"  # Checkpoint directory
  resume: null  # Resume from checkpoint (null = start fresh)
  model: "unet"  # Model architecture: "unet" or "ushape_transformer"
                 # unet: Standard U-Net autoencoder (~31M params)
                 # ushape_transformer: U-shape Transformer with CMSFFT+SGFMT (~50M params)
  amp: false  # Enable automatic mixed precision (FP16) training
              # Reduces memory usage by ~40-50%, recommended for large models/images
  gradient_checkpointing: false  # Enable gradient checkpointing for memory-efficient training
                                 # Trades compute for memory (~50-70% memory reduction, ~20-30% slower)
  optimizer_8bit: false  # Use 8-bit Adam optimizer (requires: pip install bitsandbytes)
                         # Saves ~1.5-2GB memory from optimizer states
  compile: false  # Use torch.compile for potential speedups and memory optimization
                  # Requires PyTorch 2.0+, may have longer first-epoch compilation time

# Step Control (skip completed steps)
steps:
  skip_download: false  # Skip if dataset already downloaded
  skip_prepare: false  # Skip if already prepared
  skip_crop: false  # Skip if already cropped
  prepare_only: false  # Only download/prepare/crop, don't train
